<!-- this file is automatically managed by ske validate; do not edit -->
<versions>
  <version number="1.0.0" archived-at="2025-12-07T23:57:23.437Z">
    <changelog-entry>
      <message>Before marking C7 as applied</message>
    </changelog-entry>
    <!-- Managed by ske critique mark -->
    <critiques version="1.0.0" name="2025-12-03_Feedback_Ian_Bicking">
      <critique id="C7" status="unconfirmed">
        <creator user-id="550995b6-e9a0-40b8-89a2-92940cf30ed9" email="ianbicking@gmail.com" created="2025-12-04T15:53:51 America/Chicago" name="Ian Bicking"/>
        <ref ref="/stories/Mandatory_Assistance/story-info.card" story-version="2.1.0" role="Story"/>
        <ref ref="/stories/Mandatory_Assistance/chapters/1_Impact/passages/3_Buddy_Arrives/passage.card#v2.0.0/text/progress-0.4764" passage-id="a23cb426-26cb-44e2-b1c2-7eab80537a34" role="Passage"/>
        <feedback speech="true" audio-url="https://assets.lowlight.world/feedback/c8a02296-4bad-42b9-9b69-8f92e8ffcdf0/f7445b44-c018-47ec-8c2e-6343882fe55c/9317eb81-b3af-41a2-8379-017c166f96e3.webm">
          The comment, now I'm going to die because my AI replacement wants to teach me something, is a little bit over the top, like now you're waiting for it or something, but it's not like this is going to be the fatal thing.
        </feedback>
        <reason>Tone down the hyperbole - not immediately life-threatening</reason>
      </critique>
    </critiques>
  </version>
</versions>
